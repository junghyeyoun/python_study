# -*- coding: utf-8 -*-
"""tfc_38.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kp0etxaqIuttzVQdPP4780TNXfwFicvZ
"""

# IMDB dataset으로 감성분류(이항분류) - LSTM : Dense, Conv + Dense
from keras.datasets import imdb
import numpy as np
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout, Embedding

(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=1000)
print(x_train.shape, y_train.shape, x_test.shape,y_test.shape)
print(x_train[:1])
print(y_train[:1])

len_data = [len(i) for i in x_train]
print('요소 최대 크기 : ',np.max(len_data))
print('요소 크기 평균 : ',np.mean(len_data))

plt.boxplot(len_data)
plt.show()

# x_train에 등록된 인덱스에 해당하는 단어 출력
word_to_index = imdb.get_word_index()
index_to_word = {}
for k, v in word_to_index.items():
  # print(k)
  # print(v)
  index_to_word[v + 3] = k

print(index_to_word)
print('빈도수 1등 : ',index_to_word[4]) # the
print('빈도수 100등 : ',index_to_word[104]) # after
print()
# imdb는 pad부분은 0, 문장 시작은 1, unknown은 2로 채워져 있다.
for idx, token in enumerate(('<pad>','<sos>','<unk>')):
  index_to_word[idx] = token

print(' '.join([index_to_word[i] for i in x_train[0]])) # 문장으로 변환

from keras.utils import pad_sequences
from keras.callbacks import EarlyStopping, ModelCheckpoint

max_len = 500 # 리뷰 최대 길이는 500으로 제한
x_train = pad_sequences(x_train, maxlen=max_len)
x_test = pad_sequences(x_test, maxlen=max_len)
print(x_train[:1])

# 모델 작성 방법1 : LSTM + Dense
model = Sequential()
model.add(Embedding(10001, 200, input_length=max_len))
model.add(LSTM(128, activation='tanh'))
model.add(Dense(1, activation='sigmoid'))
print(model.summary())

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])

es = EarlyStopping(monitor='val_loss', mode='auto', patience=5, baseline=0.01)
mc = ModelCheckpoint('tfc_38m1.hdf5', monitor='val_loss', save_best_only=True)

history = model.fit(x_train, y_train, validation_split=0.2, batch_size=64, epochs=100, callbacks=[es, mc], verbose=2)

print('acc : ', history.history['acc'])
print('loss : ', history.history['loss'])
print('eval : ',model.evaluate(x_test, y_test))

# pred
from keras.models import load_model
mymodel = load_model('tfc_38m1.hdf5')
pred = mymodel.predict(x_test)
print('예측값 : ',pred[:10].flatten())
print('실제값 : ',y_test[:10])

# 모델 작성 방법2 : Conv1D + Dense
from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D, Dropout

model = Sequential()
model.add(Embedding(10001, 200, input_length=max_len))
model.add(Conv1D(filters=128, kernel_size=3, padding='valid', strides=1, activation='relu'))
model.add(GlobalMaxPooling1D())
model.add(Dropout(0.3))
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
print(model.summary())

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])

es = EarlyStopping(monitor='val_loss', mode='auto', patience=5, baseline=0.01)
mc = ModelCheckpoint('tfc_38m2.hdf5', monitor='val_loss', save_best_only=True)

history = model.fit(x_train, y_train, validation_split=0.2, batch_size=64, epochs=100, callbacks=[es, mc], verbose=2)

print('acc : ', history.history['acc'])
print('loss : ', history.history['loss'])
print('eval : ',model.evaluate(x_test, y_test))

# 시각화
vloss = history.history['val_loss']
loss = history.history['loss']
epoch = np.arange(len(loss))
plt.plot(epoch, vloss, marker='.', c='red', label='val_loss')
plt.plot(epoch, loss, marker='s', c='blue', label='loss')
plt.legend()
plt.grid()
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()

# pred
from keras.models import load_model
mymodel = load_model('tfc_38m2.hdf5')
pred = mymodel.predict(x_test)
print('예측값 : ',pred[:10].flatten())
print('실제값 : ',y_test[:10])

# 새롱누 영화 평 값으로 감성 분류
import re

def sentiment_predict(new_sentence):
  new_sentence = re.sub('[^0-9a-zA-Z ]', '', new_sentence).lower()

  # 정수 인코딩
  encoded = []
  for word in new_sentence.split():
    # 단어 집합의 크기를 10,000으로 제한.
    try :
      if word_to_index[word] <= 10000:
         encoded.append(word_to_index[word]+3)
      else:
         encoded.append(2)   # 10,000 이상의 숫자는 <unk> 토큰으로 취급.
    except KeyError:
      encoded.append(2)     # 단어 집합에 없는 단어는 <unk> 토큰으로 취급.

  pad_new = pad_sequences([encoded], maxlen = max_len)  # 패딩

  # 예측하기
  score = float(mymodel.predict(pad_new))
  if(score > 0.5):
     print("{:.2f}% 확률로 긍정!.".format(score * 100))
  else:
     print("{:.2f}% 확률로 부정!".format((1 - score) * 100))

# 긍/부정 분류 예측
temp_str = "This movie was just way too overrated. The fighting was not professional."
sentiment_predict(temp_str)
temp_str = "good"
sentiment_predict(temp_str)

temp_str = "I was lucky enough to be included in the group to see the advanced screening in Seoul. And,  I need to say a big thank-you to Marvel Studios."
sentiment_predict(temp_str)
temp_str = "bad"
sentiment_predict(temp_str)