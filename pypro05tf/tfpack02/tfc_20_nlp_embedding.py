# -*- coding: utf-8 -*-
"""tfc_20_nlp_embedding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K3nJnLldIZRWBHYkeu_niKdVqINZSFxW
"""

# word embedding : 단어를 벡터로 표현하는 방식 중 하나. 희소표현과 밀집표현 등이 있다.
# 비정형화된 데이터를 숫자로 바꿔서 컴퓨터가 이해하는 형태로 번역하는 작업

import numpy as np

# 데이터 인코딩
print('레이블 인코딩')
datas = ['python','lan', 'program', 'computer','say']
datas.sort() # 사전 순으로 정렬
print(datas)
# for 사용
values=[]
for x in range(len(datas)):
  values.append(x)

print(values, type(values))

print('one-hot encoding')
onehot = np.eye(len(values))
print(onehot, type(onehot))

print('인코딩 지원 클래스')
from sklearn.preprocessing import LabelEncoder
datas = ['python','lan', 'program', 'computer','say']
encoder = LabelEncoder().fit(datas)
values = encoder.transform(datas)
print(values, type(values), np.sort(values))

print('원핫 인코딩 지원 클래스')
from sklearn.preprocessing import OneHotEncoder
labels = values.reshape(-1, 1)
print(labels, labels.shape)
onehot = OneHotEncoder().fit(labels)
onehotValues = onehot.transform(labels)
print(onehotValues.toarray())

from gensim.models import word2vec # 단어의 의미를 다차원 공간에 실수로 백터화 하는 분산표현 기법. 단어 간 유사성 표현
sentence = [['python','lan', 'program', 'computer','say']]
model = word2vec.Word2Vec(sentence, vector_size=50, min_count=1, sg=0) #CBOW, Skip-gram
print(model)

word_vectors = model.wv # 단어 벡터를 생성
print('word_vetors : ',word_vectors)
print(word_vectors.key_to_index)
print(word_vectors.key_to_index.keys())
print(word_vectors.key_to_index.values())
vocabs = word_vectors.key_to_index.keys()
word_vectors_list = [word_vectors[v] for v in vocabs]
print(word_vectors_list[0], len(word_vectors_list[0])) # 50차원

print(word_vectors.similarity(w1='python', w2='computer')) # 두 단어의 유사도를 코사인 값으로 표현. 코사인 유지도를 이용
print(word_vectors.similarity(w1='python', w2='say'))
print(word_vectors.most_similar(positive='computer'))

# 시각화
import matplotlib.pyplot as plt
def plotFunc(vocabs, x, y):
  plt.figure(figsize=(5, 4))
  plt.scatter(x, y)
  for i, v in enumerate(vocabs):
    plt.annotate(v, xy=(x[i], y[i]))

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
xys = pca.fit_transform(word_vectors_list)
xs = xys[:, 0]
ys = xys[:, 1]
plotFunc(vocabs, xs, ys)
plt.show()