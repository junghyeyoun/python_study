# -*- coding: utf-8 -*-
"""tfc_29text_generation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eGNv_Gx2M4MDgO37ZRHT_jonMhSJX6SA
"""

# 뉴욕타임스 기사의 일부 headline을 읽어 단어 단위 텍스트 생성 모델 작성
import pandas as pd

df = pd.read_csv("https://raw.githubusercontent.com/pykwon/python/master/testdata_utf8/articlesapril.csv")
print(df.head(3))
print(df.count)
print(df['headline'])

print(df['headline'].isnull().values.any())
print(df.headline.values)

headline=[]
headline.extend(list(df.headline.values))
print(headline[:10])
print(len(headline))  # 1324
if 'Unknown' in headline:print('Unknown 발견')
headline = [n for n in headline if n != 'Unknown']
print(len(headline))  # 1214
print(headline[:10])

# print('Ho하이llo 가a나다'.encode("ascii",errors="ignore").decode())
from string import punctuation
# print(", py'thon,!".strip(punctuation))
# print(", py'thon,!".strip(punctuation + ' '))

def repreFunc(s): # 다 소문자로 바꾸고, 점이나 다른 문자 빼기
  s = s.encode("utf-8").decode("ascii",errors="ignore")
  return ''.join(c for c in s if c not in punctuation).lower()

text = [repreFunc(ss) for ss in headline]
print(text[:10])

from keras.preprocessing.text import Tokenizer
tok = Tokenizer()
tok.fit_on_texts(text)
print(tok.word_index)
vocab_size = len(tok.word_index) + 1
print('단어사전(집합)의 크기 : %d'%vocab_size)

sequences = list()
for line in text:
  enc = tok.texts_to_sequences([line])[0]
  for i in range(1, len(enc)):
    se = enc[:i+1]
    sequences.append(se)
# print(sequences)
print(sequences[:5])
# ['Former N.F.L. Cheerleaders’ Settlement Offer: $1 and a Meeting With Goodell' ...
# [[99, 269], [99, 269, 371], [99, 269, 371, 1115], [99, 269, 371, 1115, 582], ...

print('dict items : ',tok.word_index.items())
index_to_word = {}
for key, value in tok.word_index.items():
  index_to_word[value] = key
print(index_to_word)
print(index_to_word[371])
print(index_to_word[1115])

max_len = max(len(i) for i in sequences)
print('배열 요소 값 중 최대 : ',max_len) # headline 중 가장 긴 단어들로 구성된 최대 수치

from keras.utils import pad_sequences
psequences = pad_sequences(sequences, maxlen=max_len, padding='pre')
print(psequences[:3])

import numpy as np
psequences = np.array(psequences)
x = psequences[:, :-1] # feature
y = psequences[:, -1] # label
print(x[:3])
print(y[:3])

# y(label)는 원핫 처리
from keras.utils import to_categorical
y = to_categorical(y, num_classes=vocab_size)
print(y[:3])

from keras.layers import Embedding, Dense, LSTM, Flatten
from keras.models import Sequential
model = Sequential()
model.add(Embedding(vocab_size, 32, input_length=max_len-1)) # 정수 인덱싱을 밀집벡터로 매핑
model.add(LSTM(128, activation='tanh'))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(vocab_size, activation='softmax'))
print(model.summary())

model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(x, y, epochs=200, verbose=2, batch_size=32)
print(model.evaluate(x,y))

# 문장 생성을 위한 함수
def textGenerationFunc(model, tok, seed_word, n):
  init_word = seed_word
  sentence = ''
  for _ in range(n):
    encoded = tok.texts_to_sequences([seed_word])[0]
    encoded = pad_sequences([encoded], maxlen=max_len - 1, padding='pre')
    result = np.argmax(model.predict(encoded, verbose=0), axis=-1)
    # 예측 단어 찾기
    for word, index in tok.word_index.items():
      # print(word, index)
      if index == result:   # 예측한 단어의 인덱스와 동일한 단어가 있다면
        break   # 해당 단어가 예측단어 이므로 break

    seed_word = seed_word + ' ' + word
    sentence = sentence + ' ' + word
  sentence = init_word + sentence
  return sentence

print(textGenerationFunc(model, tok, 'the', 10))
print(textGenerationFunc(model, tok, 'the', 100))
print(textGenerationFunc(model, tok, 'changed leaders looking', 50))