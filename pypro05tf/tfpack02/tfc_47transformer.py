# -*- coding: utf-8 -*-
"""tfc_47Transformer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TfeGoy3VXGn_2PmPL07bydaOCP1uXuAc
"""

# Transformer : RNN을 제거하고 Attention으로만 구성된 서로 다른 인코더와 디코더가 여러 개 쌓인 형태의 네트워크
# 인코더 또는 디코더 안에는 self attention과 feedford 신경망으로 구성되어 있다.
# self attention 함수는 주어진 Query에 대해서 key와 유사도를 구한다. 그리고 이 유사도를 키와 매핑되어 있는 값(value)에 반영한다.
# self attention은 RNN처럼 순서대로 처리 하는 방식이 아니라 해당하는 단어와 관련된 뜻을 찾기 위한 어텐션을 말한다.

# 간단한 구조 이해
import tensorflow as tf
from tensorflow import keras
from keras import layers
from keras.models import Sequential

# load data
vocab_size = 20000
maxlen = 200

(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)
print(len(x_train), len(x_val))

x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)
x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)
print(x_train)

# Token and Position embedding
inputs = layers.Input(shape=(200,))
x = layers.Embedding(input_dim=128, output_dim=32)(inputs)
print(tf.keras.Model(inputs=inputs, outputs=x).summary())

inputs = layers.Input(shape=(200,))
x = layers.Embedding(input_dim=128, output_dim=32)(inputs)
positions = tf.range(start=0, limit=200, delta=1)
# 각 단어 위치에 따른 index값을 주고 embedding하여 feature를 추출한 후, 단어embedding결과 + 위치embedding결과 합을 구함
positions = layers.Embedding(input_dim=200, output_dim=32)(positions)
embedding_layer = x + positions

# Multi-head attention을 위한 간단한 구조를 작성
EmbeddingDim = 5
WordLen = 5
inputs = layers.Input(shape=(WordLen,))
positions = tf.range(start=0, limit=WordLen, delta=1)
positions = layers.Embedding(input_dim=WordLen, output_dim=EmbeddingDim)(positions)
x = layers.Embedding(input_dim=200, output_dim=EmbeddingDim)(inputs)
embedding_layer = x + positions

# num_heads = 1 : 현재 Attention Matrix를 만드는 작업을 몇 번할 것인가를 결정
attention_output = layers.MultiHeadAttention(num_heads=1, key_dim=1, use_bias=False)(embedding_layer, embedding_layer)
print(tf.keras.Model(inputs=inputs, outputs=x).summary())

model = tf.keras.Model(inputs=inputs, outputs=attention_output)
print([w.shape for w in model.get_weights()])
# [(200, 5),            (5, 1, 1), (5, 1, 1), (5, 1, 1),                  (1, 1, 5)]
# embedding_layer weight, Query,    Key,     Value에 대한 weight, dot_production내부에서 transpose되어 순서가 변경
# use_bias = True하면 중간에 상수가 더해짐
# 파라미터에 대한 존재 이유, 연산 등의 이해가 있다면, 다음으로 Scaled dot production Attention만 이해하면 Transformer에 기초
# 단어들의 유사도와 같은 역할을 하는 Attention score를 구하고 이를 Value에 곱해주는 작업을 Single-Head Attention이 한다.
# Single-Head Attention을 여러개 병렬로 처리하면 이게 바로 MultiHeadAttention이 된다.
# 이때 num_heads는 이 Attention matrix를 만드는 수행을 몇 번 할 것인가를 의미한다.

# 이제 MultiHeadAttention을 통과한 분류 모델을 작성
EmbeddingDim = 128
WordLen = 200

inputs = layers.Input(shape=(WordLen,))
positions = tf.range(start=0, limit=WordLen, delta=1)
positions = layers.Embedding(input_dim=WordLen, output_dim=EmbeddingDim)(positions)
x = layers.Embedding(input_dim=200, output_dim=EmbeddingDim)(inputs)
embedding_layer = x + positions

attention_output = layers.MultiHeadAttention(num_heads=1, key_dim=2, use_bias=True)(embedding_layer, embedding_layer)
x = layers.GlobalAveragePooling1D()(attention_output)
# MultiHeadAttention을 통과한 output은 Embedding layer의 output과 동일하므로 GlobalAveragePooling1D을 통해 Vector화 할 수 있다
outputs=layers.Dense(2, activation='softmax')(x)
print(tf.keras.Model(inputs=inputs, outputs=attention_output).summary())

model = tf.keras.Model(inputs=inputs, outputs=outputs)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])
print(model.summary())

