# -*- coding: utf-8 -*-
"""tfc_25_count.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ONBLUpyXKqyEp7lHkrh1FD_EiOX7Mtbo
"""

# 자연어 처리에서 특징 추출이란 단어나 문장들을 어떤 특정 값으로 변환하는 것을 의미한다.
# 문자로 구성된 데이터를 모델에 적용할 수 있도록 특징을 추출해 수치화한다.
# 단어의 수를 파악해 문장을 분석하는 방법

from sklearn.feature_extraction.text import CountVectorizer

textData = ['나는 배 고프다 아니 배가 고프다.','내일 점심 뭐 먹지?','내일 공부 해야겠다.','점심 먹고 공부 해야지!']
count_vec = CountVectorizer(analyzer='word',ngram_range=(1,1),stop_words=['나는']) # ngram_range -> 단어장 생성에 사용할 토큰 크기를 결정
count_vec.fit(textData)
print(count_vec.get_feature_names_out()) # 구두점 x, 한글자 x
print(count_vec.vocabulary_) # 사전순으로 인덱싱
print([textData[0]])
sentence = [textData[0]]
print(count_vec.transform(sentence)) # 벡터화
print(count_vec.transform(sentence).toarray())

# TF_IDF : Term Frequency(1개의 문서 내 특정 단어 등장 빈도) - Inverse Document Frequency
# DF : 특정 단어가 나타나는 문장 수
# 정보 검색과 텍스트 마이닝에서 이용하는 가중치로, 여러 문서로 이루어진 문서군이 있을 때 어떤 단어가 특정 문서 내에서 얼마나 중요한 것인지를 나타내는 통계적 수치이다
# 단순히 빈도수로 그 단어의 가치를 정하는 것이 아니라, 여러 문장에 많이 등장하는 단어는 패널티를 주어 단어 빈도의 스케일을 맞추는 기법이다.

from sklearn.feature_extraction.text import TfidfVectorizer

textData = ['나는 배 고프다 아니 배가 고프다.','내일 점심 뭐 먹지?','내일 공부 해야겠다.','점심 먹고 공부 해야지!']
tfidf_vec = TfidfVectorizer(analyzer='word',ngram_range=(1,1),stop_words=['나는'])
tfidf_vec.fit(textData)
print(tfidf_vec.get_feature_names_out())
print(tfidf_vec.vocabulary_)
print(tfidf_vec.transform(textData).toarray())
print()
sentence = [textData[3]]
print(sentence)
print(tfidf_vec.transform(sentence))
print(tfidf_vec.transform(sentence).toarray())

# TfidfVectorizer를 사용해 *형태소 분석기 Okt 를 적용)를 벡터로 변환 후 단어 간 유사도 계산
!pip install konlpy
from konlpy.tag import Okt
from sklearn.metrics.pairwise import cosine_similarity

okt =Okt()
def tokenizeFunc(ss):  # 형태소 분석용 함수
  ss = okt.normalize(ss)  # Okt는 정규화를 일부 지원함. 예) '사랑햌' => '사랑해'로 수정
  ss = okt.morphs(ss)  # 형태소 단위로 분리. 반환형은 리스트
  return ss

texts = ['길동이는 파이썬을 좋아합니다', '길동이는 웹을 잘합니다', '길동이는 운동을 매우 잘합니다']
new_texts = ['길동이는 파이썬을 좋아하고 운동을 잘합니다.']

tfidf = TfidfVectorizer(tokenizer = tokenizeFunc, token_pattern=None).fit(texts)
print(tfidf.vocabulary_)
tfidf_matrix = tfidf.fit_transform(texts)
print(tfidf_matrix)
print(tfidf_matrix.toarray())

for ntext in new_texts:
  tftrans = tfidf.transform([ntext])  # 새로운 문장을 벡터로 변환 '길동이는 파이썬을 좋아하고 운동을 잘합니다'
  print('tftrans : \n', tftrans)
  # 새로운 문장과 기존 문자들 사이의 코사인 유사도 계산(데이터 크기 차이에 관계없이 계산)
  cosine_simil = cosine_similarity(tftrans, tfidf_matrix)
  print('cosine_simil : ', cosine_simil)

  # 출력
  print (f'새로운 문장 : {ntext}')
  print('------')
  print(f'기존문장 : ')
  for idx in range(3):
    print(cosine_simil.argsort()[0])  # [0]은 2차원 배열의 0행을 의미
    print((idx+1)*-1)
    print(cosine_simil[0][(idx+1)*-1])
    most_simil_idx = cosine_simil.argsort()[0][(idx+1)*-1]
    print(most_simil_idx)
    most_simil_sentence = texts[most_simil_idx]
    simil_score = cosine_simil[0][most_simil_idx]
    print(f'{most_simil_sentence} (유사도:{simil_score:.3f})')
print()