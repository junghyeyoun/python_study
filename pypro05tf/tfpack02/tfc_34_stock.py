# -*- coding: utf-8 -*-
"""tfc_34_stock.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NojWAmPHt-4Jwij8EwgSHKv9nxfyKzA8
"""

# LSTM + Dense 모델을 작성해 주가 예측
# 삼성전자 증권 표준코드 005930
!pip install finance-datareader

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import FinanceDataReader as fdr
import warnings
warnings.filterwarnings('ignore')

STOCK_CODE = '005930'
stock_data = fdr.DataReader(STOCK_CODE)
print(stock_data.head(3))
print(stock_data.tail(3))

print('상관관계 : ',stock_data.corr())
stock_data.reset_index(inplace=True)
print(stock_data.head(3))
stock_data.drop(['Change'], axis='columns', inplace=True)

# Date열을 이용해 연도별 주가 변동 시각화
# Date열을 연,월,일로 분리해 새로운 열을 추가
stock_data['year'] = stock_data['Date'].dt.year
stock_data['month'] = stock_data['Date'].dt.month
stock_data['day'] = stock_data['Date'].dt.day
print(stock_data.head(3))
print(stock_data.shape)  # (6000, 9)

df = stock_data.loc[stock_data['year'] >= 2000]
plt.figure(figsize=(6, 4))
sns.lineplot(y=df['Close'], x=df.year)
plt.xlabel('time')
plt.ylabel('price')
plt.show()

# 전처리 계속 : Opne High Low Close Volume
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scale_cols = ['Open', 'High', 'Low', 'Close', 'Volume']
df_scaled = scaler.fit_transform(stock_data[scale_cols])
df_scaled = pd.DataFrame(df_scaled)
df_scaled.columns = scale_cols
print(df_scaled.head(3))

col_close = ['Close']
close_scaled = scaler.fit_transform(stock_data[col_close])
print('스케일 값 :', close_scaled[:5].ravel())
print('스케일 원복 값 :', scaler.inverse_transform(close_scaled[:5]).ravel())

# 과거 20일을 기준으로 그 다음날의 종가를 예측, 전체 데이터는 과거 200일 기준. 예측 기준은 과거 20일 사용
TEST_SIZE = 200
train = df_scaled[:-TEST_SIZE]
test = df_scaled[-TEST_SIZE:]
print(train.shape)  # (5800, 4)
print(test.shape)   # (200, 4)

# dataset 작성 함수
def make_dataset_func(data, label, window_size = 20):
  feature_list = []
  label_list = []
  for i in range(len(data) - window_size):
    feature_list.append(np.array(data.iloc[i:i + window_size]))
    label_list.append(np.array(label.iloc[i + window_size]))
  return np.array(feature_list), np.array(label_list)   # 순차적으로 20일 동안의 데이터를 묶어, 대응하는 label과 함께 반환

feature_cols = ['Open', 'High', 'Low', 'Volume']
label_cols = ['Close']

train_feature = train[feature_cols]
train_label = train[label_cols]
test_feature = test[feature_cols]
test_label = test[label_cols]
print(train_feature[:3])
print(train_label[:3])
print(train_feature.shape, train_label.shape, test_feature.shape, test_label.shape) # (5800, 3) (5800, 1) (200, 3) (200, 1)

train_feature, train_label = make_dataset_func(train_feature, train_label)
print(train_feature[:2], train_label[:2])

# train_test_split
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(train_feature, train_label, test_size=0.2, random_state=12, shuffle=False)
print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)   # (4624, 20, 4) (1156, 20, 4) (4624, 1) (1156, 1)

# test
test_feature, test_label = make_dataset_func(test_feature, test_label)
print(test_feature[:2], test_label[:2])
print(test_feature.shape, test_label.shape) # (180, 20, 4) (180, 1)

# model
from keras.models import Sequential
from keras.layers import Dense, LSTM
from keras.callbacks import EarlyStopping, ModelCheckpoint

model = Sequential()
model.add(LSTM(units=16, activation='tanh', input_shape=(train_feature.shape[1], train_feature.shape[2]), return_sequences=False)) # many-to-one
model.add(Dense(units=16, activation='relu'))
model.add(Dense(units=1))
print(model.summary())

from keras.losses import Huber
model.compile(optimizer='adam', loss=Huber(), metrics=['mse'])
# Huber loss는 지점에서 미분이 가능하면서 이상치에 강건한(robust) 성격을 보이는 loss function 이다.

es = EarlyStopping(monitor='val_loss', patience=10)
chkPoint = ModelCheckpoint('tfc_34.hdf5', monitor='val_loss', save_best_only=True, mode='auto', verbose=0)
history = model.fit(x_train, y_train, epochs=100, batch_size=8, validation_data=(x_test, y_test), verbose=2, callbacks=[es, chkPoint])

plt.figure(figsize=(6,4))
plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label='val_loss', c='red')
plt.legend()
plt.show()

import keras
model = keras.models.load_model('tfc_34.hdf5')

pred = model.predict(test_feature, verbose=0)

from sklearn.metrics import r2_score
# print('r2_score : ',r2_score(test_label, pred))

# 정규화한 값
print('pred : ',np.round(pred[:10].flatten(), 2))
print('real : ',np.round(test_label[:10].flatten(), 2))
# 원복
print('pred : ',scaler.inverse_transform(pred[:10]).flatten())
print('real : ', scaler.inverse_transform(test_label[:10]).flatten())

# 실제 값, 예측 값 시각화
plt.figure(figsize=(6,4))
plt.plot(test_label[:20], label='real')
plt.plot(pred[:20], label='pred', c='red')
plt.legend()
plt.show()

