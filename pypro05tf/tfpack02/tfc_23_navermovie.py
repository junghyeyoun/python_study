# -*- coding: utf-8 -*-
"""tfc_23_navermovie.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xkL-9Bfp7vTbV5LwBb43oPo6Myg42E4C
"""

# 네이버 제공 영화 리뷰 데이터를 이용해 단어 간 유사도 확인
!pip install konlpy
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from konlpy.tag import Okt
from gensim.models.word2vec import Word2Vec

import urllib
urllib.request.urlretrieve("https://raw.githubusercontent.com/pykwon/python/master/testdata_utf8/ratings_train.txt", filename='ratings.txt')
train_data = pd.read_table('ratings.txt')
print(train_data[:5], len(train_data))  # 150000
print(train_data.isnull().values.any())
print(train_data.info())
train_data = train_data.dropna()
print(len(train_data))

train_data['document'] = train_data['document'].str.replace("[^가-힣 ]","") # 정규 표현식
print(train_data[:5])

# 불용어(stop words) : 의미가 없는(분석에 의미가 없는) 단어 token
# 영문의 경우에는 nltk에 이미 불용어 사전으로 등록이 되어 있으나 한글은 매우 동적이므로 불용어 사전이 없음
stopwords = ['의','가','는','은','잘','을','를','으로','하여','에','와','한','하다','앗','아','그래서']

okt = Okt()
tokenized_data = []
for sen in train_data['document']:
  temp = okt.morphs(sen, stem=True) # stem=True -> 어근 처리
  temp = [word for word in temp if not word in stopwords]
  tokenized_data.append(temp)

print('글 최대 길이 : ',max(len(i) for i in tokenized_data))
print('글 평균 길이 : ',sum(map(len,tokenized_data))/len(tokenized_data))

plt.hist([len(i) for i in tokenized_data], bins=50)
plt.xlabel('length')
plt.ylabel('count')
plt.show()

model = Word2Vec(sentences=tokenized_data, vector_size=100, window=10, min_count=3, sg=0)

print(model.wv.vectors.shape)
print(model.wv.most_similar('마동석'))

