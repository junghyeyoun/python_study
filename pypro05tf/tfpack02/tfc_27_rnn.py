# -*- coding: utf-8 -*-
"""tfc_27_rnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CqY_S3-o9puj56ws4ElHjZ4fr0T4O-tT
"""

# 문자열 토큰 처리 후 LSTM으로 감성분류(?)
from keras.preprocessing.text import Tokenizer
import numpy as np
from keras.models import Sequential
from keras.layers import SimpleRNN, LSTM, Dense

samples = ['The cat say on the mat.','The dog ate my homework.']

token_index = {}
for sam in samples:
  for word in sam.split(sep=' '):
    if word not in token_index:
      token_index[word] = len(token_index)
print(token_index)
print()
tokenizer = Tokenizer(num_words=10)
tokenizer.fit_on_texts(samples)
token_seq = tokenizer.texts_to_sequences(samples) # 텍스트를 정수 인덱싱해서 리스트로 반환
print(token_seq)
print(tokenizer.word_index)
print()
token_mat = tokenizer.texts_to_matrix(samples, mode='binary')
print(token_mat) # 원핫 이진 벡터
print(tokenizer.word_counts)
print(tokenizer.document_count)
print(tokenizer.word_docs)

from keras.utils import to_categorical
token_seq = to_categorical(token_seq[0], num_classes=6)
print(token_seq)

docs = ['너무 재밌네요.','최고예요','참 잘 만든 영화에요', '추천하고싶은 영화입니다.','한 번 더 보고싶어요.','글쎄요','별로에요','생각보다 지루하네요','연기가 어색해요','재미없어요']
labels = np.array([1,1,1,1,1,0,0,0,0,0])

token = Tokenizer()
token.fit_on_texts(docs)
print(token.word_index)

x = token.texts_to_sequences(docs)
print('정수 인덱싱된 트큰 결과 : ',x)

from keras.utils import pad_sequences # 서로 다른 크기의 데이터를 일정한 크기로 만듦
from keras.layers import Embedding, Flatten

padded_x = pad_sequences(x, 5)
print(padded_x)

word_size = len(token.word_index) +1 # 임배딩에 입력될 단어 수 (토큰 수)를 지정. 가능한 토큰 수는 최대값 + 1을 준다.
model = Sequential()
# Embedding(가능 토큰수, 벡터 크기, input_length=시퀀스 개수) - 워드 임배딩이란 텍스트 내의 단어들을 밀집 벡터로 만드는 것
# 워드 임베딩 작업을 수행하고(number of samples, embedding word dimensionality, input_length)만 3D 텐서를 리턴합니다.
model.add(Embedding(word_size, 8, input_length=5)) # RNN layer
model.add(LSTM(units=32, activation='tanh'))
model.add(Flatten())
model.add(Dense(units=32, activation='relu')) # Dense layer
model.add(Dense(units=1, activation='sigmoid'))
print(model.summary())

model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])
model.fit(padded_x, labels, epochs=20, verbose=1)
print('eval acc : %.4f'%(model.evaluate(padded_x, labels)[1]))

print('predict : ',np.where(model.predict(padded_x)>0.5, 1, 0).ravel())
print('read : ',labels)